# @package _global_

defaults:
  - /dataset/dclm@train_dataset  # seq len 2048

val_log_interval: 250

train_dataset:
  num_steps: 4000
  global_batch_size: 512  # about 4b tokens

val_list:
  - dataset:
      name: DCLM
      path: ${train_dataset.path}
      split: VAL
      seq_len: ${train_dataset.seq_len}
      global_batch_size: 256  # 512K tokens
      num_steps: 64  # about 34M tokens
    evaluator: NLL

eval_list:
  - dataset:
      name: DCLM
      path: ${train_dataset.path}
      split: VAL
      seq_len: 1024  # At inference time, this functions like prompt size
      global_batch_size: ${sharding.mesh_shape[0]}
      use_splash: False
    evaluator: AUTOREGRESSIVE_ROLLOUTS

model:
  transformer_type: DISCRETE
  max_seq_len: ${train_dataset.seq_len}  # no longer than train
  is_causal: True
  num_vocab: 131072  # llama 3 tokenizer, rounded up to 128K

# TODO: tune hparams, find good defaults!
optimizer:
  weight_decay: 1e-1

inference:
  tokenizer: LLAMA3
  max_tokens_to_generate: 1024
