# @package _global_

defaults:
  - /dataset/dclm@train_dataset  # seq len 2048

val_log_interval: 250
# These need to be tuned fairly aggressively wrt d_model
fused_xent_block_size_T: 512
fused_xent_block_size_V: 256
fused_xent_block_size_V_compute: 256

train_dataset:
  num_steps: 4000
  global_batch_size: 512  # about 4b tokens
  use_splash: True
  splash_block_size_q: 1024
  splash_block_size_kv: 512
  splash_block_size_kv_compute: 512

val_list:
  - dataset:
      name: DCLM
      path: ${train_dataset.path}
      split: VAL
      seq_len: ${train_dataset.seq_len}
      global_batch_size: 256  # 512K tokens
      num_steps: 64  # about 34M tokens
      use_splash: True
      splash_block_size_q: ${train_dataset.splash_block_size_q}
      splash_block_size_kv: ${train_dataset.splash_block_size_kv}
      splash_block_size_kv_compute: ${train_dataset.splash_block_size_kv_compute}
    evaluator: NLL

eval_list:
  - dataset:
      name: DCLM
      path: ${train_dataset.path}
      split: VAL
      seq_len: 1024  # At inference time, this functions like prompt size
      global_batch_size: ${sharding.mesh_shape[0]}
      use_splash: False
    evaluator: AUTOREGRESSIVE_ROLLOUTS

model:
  transformer_type: DISCRETE
  max_seq_len: ${train_dataset.seq_len}  # no longer than train
  is_causal: True
  num_vocab: 131072  # llama 3 tokenizer, rounded up to 128K

# TODO: tune hparams, find good defaults!
optimizer:
  weight_decay: 1e-1

inference:
  tokenizer: LLAMA3
  max_tokens_to_generate: 1023
