# @package _global_

defaults:
  - /dataset/mnist@train_dataset


val_log_interval: 468 # every epoch
use_fused_xent_loss: False

train_dataset:
  path: "data/mnist/patch_7x7"
  split: TRAIN
  seq_len: 16  # 7x7 patches
  global_batch_size: 128
  epochs_to_loop: 15
  num_steps: 7020  # hardcoded: 15 epochs, 128 batch, 60k train set size (drop last)
  use_splash: False

val_list:
  - dataset:
      name: MNIST
      path: "data/mnist/patch_7x7"
      split: VAL
      seq_len: 16
      global_batch_size: 4000
      epochs_to_loop: 1
      use_splash: False
    evaluator: ACCURACY
  - dataset:
      name: MNIST
      path: "data/mnist/patch_7x7"
      split: VAL
      seq_len: 16
      global_batch_size: 4000
      epochs_to_loop: 1
      use_splash: False
    evaluator: NLL
  - dataset:
      name: MNIST
      path: "data/mnist/patch_7x7"
      split: VAL
      seq_len: 16
      global_batch_size: 4000
      epochs_to_loop: 1
      use_splash: False
    evaluator: PERPLEXITY

eval_list:
  - dataset:
      name: MNIST
      path: "data/mnist/patch_7x7"
      split: TEST
      seq_len: 16
      global_batch_size: 32
      epochs_to_loop: 1
      use_splash: False
    evaluator: ACCURACY
  - dataset:
      name: MNIST
      path: "data/mnist/patch_7x7"
      split: TRAIN
      seq_len: 16
      global_batch_size: 60000
      epochs_to_loop: 1
      use_splash: False
    evaluator: ACCURACY

model:
  transformer_type: CONTINUOUS
  is_causal: False  # use full attention
  num_vocab: 49  # 7x7 patches
  num_classes: 10
  use_bias_embeddings: True
  use_bias_ln: False
  use_bias_mlp: False
  # num_layers: 3
  # num_heads: 8
  # d_model: 64
  # d_head: 8
  # mlp_factor: 256


optimizer:
  weight_decay: 1e-4
