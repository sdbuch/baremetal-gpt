# @package _global_

# follows Qwen2.5-1.5B config
# But no GQA!
model:
  d_model: 1536
  num_heads: 12
  d_head: 128
  num_layers: 28
  mlp_factor: 5.83333333333333333333333333333

  use_gating_mlp: True
