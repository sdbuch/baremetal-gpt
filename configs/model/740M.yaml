# @package _global_

# about 740M params with DCLM tokenizer embeddings (& biases)
model:
  d_model: 3072
  num_heads: 24
  d_head: 128
  num_layers: 12

  use_gating_mlp: True
