# @package _global_

# about 570M params with DCLM tokenizer embeddings (& biases)
# Follows GPT2-Med (but with gating)
model:
  d_model: 1024
  num_heads: 16
  d_head: 64
  num_layers: 24

  use_gating_mlp: True
