# @package _global_

# about 1.8B params with DCLM tokenizer embeddings (& biases)
# Follows GPT2-XL (but with gating)
model:
  d_model: 1536
  num_heads: 24
  num_layers: 48

  use_gating_mlp: True
